<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2026-01-01">

<title>一文了解VLM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../../site_libs/bootstrap/bootstrap-dark-0d4a9d19931feb4fee1ddab4b2489000.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../pages/index.html"> 
<span class="menu-text"><span class="lang-en">About Me</span><span class="lang-zh">关于我</span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../pages/home.html"> 
<span class="menu-text"><span class="lang-en">Home</span><span class="lang-zh">主页</span></span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../content/blogs/index.html"> 
<span class="menu-text"><span class="lang-en">Blogs</span><span class="lang-zh">博客</span></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/projects/index.html"> 
<span class="menu-text"><span class="lang-en">Projects</span><span class="lang-zh">项目</span></span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-span-classlang-enpapersreportsspanspan-classlang-zh论文和技术报告span" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text"><span class="lang-en">Papers&amp;Reports</span><span class="lang-zh">论文和技术报告</span></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-span-classlang-enpapersreportsspanspan-classlang-zh论文和技术报告span">    
        <li>
    <a class="dropdown-item" href="../../../content/papers/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../content/papers/deepseek/index.html">
 <span class="dropdown-text">DeepSeek</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../content/papers/labnotes/index.html">
 <span class="dropdown-text">LabNotes</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-span-classlang-enlearning-notesspanspan-classlang-zh学习笔记span" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text"><span class="lang-en">Learning Notes</span><span class="lang-zh">学习笔记</span></span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-span-classlang-enlearning-notesspanspan-classlang-zh学习笔记span">    
        <li>
    <a class="dropdown-item" href="../../../content/learning-notes/index.html">
 <span class="dropdown-text">Overview</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../content/learning-notes/golang/index.html">
 <span class="dropdown-text">golang基础</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../content/learning-notes/microservices/index.html">
 <span class="dropdown-text">微服务(golang)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../content/learning-notes/infrastructure/index.html">
 <span class="dropdown-text">基础设施(golang)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../content/learning-notes/deepL/index.html">
 <span class="dropdown-text">深度学习</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/aorangehc" title="" class="quarto-navigation-tool px-1" aria-label="GitHub"><i class="bi bi-github"></i></a>
    <a href="mailto:morange0205@gmail.com" title="" class="quarto-navigation-tool px-1" aria-label="Email"><i class="bi bi-envelope"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#参考资料" id="toc-参考资料" class="nav-link active" data-scroll-target="#参考资料">参考资料</a></li>
  <li><a href="#一文了解-vlm" id="toc-一文了解-vlm" class="nav-link" data-scroll-target="#一文了解-vlm">一文了解 VLM</a>
  <ul class="collapse">
  <li><a href="#一什么是视觉语言模型" id="toc-一什么是视觉语言模型" class="nav-link" data-scroll-target="#一什么是视觉语言模型">一、什么是视觉语言模型？</a>
  <ul class="collapse">
  <li><a href="#通俗理解给-ai-装上眼睛" id="toc-通俗理解给-ai-装上眼睛" class="nav-link" data-scroll-target="#通俗理解给-ai-装上眼睛">1.1 通俗理解：给 AI 装上”眼睛”</a></li>
  <li><a href="#技术定义多模态-ai-的集大成者" id="toc-技术定义多模态-ai-的集大成者" class="nav-link" data-scroll-target="#技术定义多模态-ai-的集大成者">1.2 技术定义：多模态 AI 的集大成者</a></li>
  <li><a href="#vlm-与纯视觉模型纯语言模型的区别" id="toc-vlm-与纯视觉模型纯语言模型的区别" class="nav-link" data-scroll-target="#vlm-与纯视觉模型纯语言模型的区别">1.3 VLM 与纯视觉模型、纯语言模型的区别</a></li>
  </ul></li>
  <li><a href="#二vlm-的发展历程从萌芽到爆发" id="toc-二vlm-的发展历程从萌芽到爆发" class="nav-link" data-scroll-target="#二vlm-的发展历程从萌芽到爆发">二、VLM 的发展历程：从萌芽到爆发</a>
  <ul class="collapse">
  <li><a href="#早期探索2019-2020双塔结构的兴起" id="toc-早期探索2019-2020双塔结构的兴起" class="nav-link" data-scroll-target="#早期探索2019-2020双塔结构的兴起">2.1 早期探索（2019-2020）：双塔结构的兴起</a></li>
  <li><a href="#clip-时代2021对比学习的革命" id="toc-clip-时代2021对比学习的革命" class="nav-link" data-scroll-target="#clip-时代2021对比学习的革命">2.2 CLIP 时代（2021）：对比学习的革命</a></li>
  <li><a href="#统一框架2022理解与生成的一体化" id="toc-统一框架2022理解与生成的一体化" class="nav-link" data-scroll-target="#统一框架2022理解与生成的一体化">2.3 统一框架（2022）：理解与生成的一体化</a></li>
  <li><a href="#大模型时代2023llm-赋能的视觉助手" id="toc-大模型时代2023llm-赋能的视觉助手" class="nav-link" data-scroll-target="#大模型时代2023llm-赋能的视觉助手">2.4 大模型时代（2023）：LLM 赋能的视觉助手</a></li>
  <li><a href="#最新进展2024-2025多模态原生大模型" id="toc-最新进展2024-2025多模态原生大模型" class="nav-link" data-scroll-target="#最新进展2024-2025多模态原生大模型">2.5 最新进展（2024-2025）：多模态原生大模型</a></li>
  </ul></li>
  <li><a href="#三vlm-的核心技术原理" id="toc-三vlm-的核心技术原理" class="nav-link" data-scroll-target="#三vlm-的核心技术原理">三、VLM 的核心技术原理</a>
  <ul class="collapse">
  <li><a href="#模型架构单流-vs-双流" id="toc-模型架构单流-vs-双流" class="nav-link" data-scroll-target="#模型架构单流-vs-双流">3.1 模型架构：单流 vs 双流</a></li>
  <li><a href="#预训练目标" id="toc-预训练目标" class="nav-link" data-scroll-target="#预训练目标">3.2 预训练目标</a></li>
  <li><a href="#视觉特征提取的演进" id="toc-视觉特征提取的演进" class="nav-link" data-scroll-target="#视觉特征提取的演进">3.3 视觉特征提取的演进</a></li>
  <li><a href="#训练策略两阶段-vs-三阶段" id="toc-训练策略两阶段-vs-三阶段" class="nav-link" data-scroll-target="#训练策略两阶段-vs-三阶段">3.4 训练策略：两阶段 vs 三阶段</a></li>
  </ul></li>
  <li><a href="#四主流-vlm-模型详解" id="toc-四主流-vlm-模型详解" class="nav-link" data-scroll-target="#四主流-vlm-模型详解">四、主流 VLM 模型详解</a>
  <ul class="collapse">
  <li><a href="#clip跨模态对比学习的开创者" id="toc-clip跨模态对比学习的开创者" class="nav-link" data-scroll-target="#clip跨模态对比学习的开创者">4.1 CLIP：跨模态对比学习的开创者</a></li>
  <li><a href="#blipblip-2统一理解与生成" id="toc-blipblip-2统一理解与生成" class="nav-link" data-scroll-target="#blipblip-2统一理解与生成">4.2 BLIP/BLIP-2：统一理解与生成</a></li>
  <li><a href="#llava视觉指令微调的先锋" id="toc-llava视觉指令微调的先锋" class="nav-link" data-scroll-target="#llava视觉指令微调的先锋">4.3 LLaVA：视觉指令微调的先锋</a></li>
  <li><a href="#minigpt-4极简设计的强大能力" id="toc-minigpt-4极简设计的强大能力" class="nav-link" data-scroll-target="#minigpt-4极简设计的强大能力">4.4 MiniGPT-4：极简设计的强大能力</a></li>
  <li><a href="#qwen-vl多语言多图像的通用模型" id="toc-qwen-vl多语言多图像的通用模型" class="nav-link" data-scroll-target="#qwen-vl多语言多图像的通用模型">4.5 Qwen-VL：多语言多图像的通用模型</a></li>
  </ul></li>
  <li><a href="#五vlm-的应用场景" id="toc-五vlm-的应用场景" class="nav-link" data-scroll-target="#五vlm-的应用场景">五、VLM 的应用场景</a>
  <ul class="collapse">
  <li><a href="#图像描述与视觉问答" id="toc-图像描述与视觉问答" class="nav-link" data-scroll-target="#图像描述与视觉问答">5.1 图像描述与视觉问答</a></li>
  <li><a href="#图文检索" id="toc-图文检索" class="nav-link" data-scroll-target="#图文检索">5.2 图文检索</a></li>
  <li><a href="#文档理解" id="toc-文档理解" class="nav-link" data-scroll-target="#文档理解">5.3 文档理解</a></li>
  <li><a href="#视觉定位与指代表达" id="toc-视觉定位与指代表达" class="nav-link" data-scroll-target="#视觉定位与指代表达">5.4 视觉定位与指代表达</a></li>
  <li><a href="#创意生成" id="toc-创意生成" class="nav-link" data-scroll-target="#创意生成">5.5 创意生成</a></li>
  </ul></li>
  <li><a href="#六幻觉问题vlm-的阿喀琉斯之踵" id="toc-六幻觉问题vlm-的阿喀琉斯之踵" class="nav-link" data-scroll-target="#六幻觉问题vlm-的阿喀琉斯之踵">六、幻觉问题：VLM 的阿喀琉斯之踵</a>
  <ul class="collapse">
  <li><a href="#什么是-vlm-幻觉" id="toc-什么是-vlm-幻觉" class="nav-link" data-scroll-target="#什么是-vlm-幻觉">6.1 什么是 VLM 幻觉？</a></li>
  <li><a href="#幻觉的分类" id="toc-幻觉的分类" class="nav-link" data-scroll-target="#幻觉的分类">6.2 幻觉的分类</a></li>
  <li><a href="#幻觉的成因" id="toc-幻觉的成因" class="nav-link" data-scroll-target="#幻觉的成因">6.3 幻觉的成因</a></li>
  <li><a href="#幻觉的评估" id="toc-幻觉的评估" class="nav-link" data-scroll-target="#幻觉的评估">6.4 幻觉的评估</a></li>
  <li><a href="#幻觉的治理" id="toc-幻觉的治理" class="nav-link" data-scroll-target="#幻觉的治理">6.5 幻觉的治理</a></li>
  </ul></li>
  <li><a href="#七vlm-的技术挑战与未来方向" id="toc-七vlm-的技术挑战与未来方向" class="nav-link" data-scroll-target="#七vlm-的技术挑战与未来方向">七、VLM 的技术挑战与未来方向</a>
  <ul class="collapse">
  <li><a href="#当前挑战" id="toc-当前挑战" class="nav-link" data-scroll-target="#当前挑战">7.1 当前挑战</a></li>
  <li><a href="#未来方向" id="toc-未来方向" class="nav-link" data-scroll-target="#未来方向">7.2 未来方向</a></li>
  </ul></li>
  <li><a href="#八总结" id="toc-八总结" class="nav-link" data-scroll-target="#八总结">八、总结</a></li>
  <li><a href="#参考资料-1" id="toc-参考资料-1" class="nav-link" data-scroll-target="#参考资料-1">参考资料</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">一文了解VLM</h1>
  <div class="quarto-categories">
    <div class="quarto-category">VLM</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 1, 2026</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="../assets/images/allVLM/image.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
<p>VLM（Vision-Language Model）: 视觉语言大模型</p>
<section id="参考资料" class="level2">
<h2 class="anchored" data-anchor-id="参考资料">参考资料</h2>
</section>
<section id="一文了解-vlm" class="level1">
<h1>一文了解 VLM</h1>
<blockquote class="blockquote">
<p>视觉语言模型（Vision-Language Model，VLM）是人工智能领域最激动人心的进展之一。它让机器能够同时”看懂”图像和”理解”语言，开启了人机交互的全新范式。本文将带你从零开始，全面了解 VLM 的原理、发展与未来。</p>
</blockquote>
<hr>
<section id="一什么是视觉语言模型" class="level2">
<h2 class="anchored" data-anchor-id="一什么是视觉语言模型">一、什么是视觉语言模型？</h2>
<section id="通俗理解给-ai-装上眼睛" class="level3">
<h3 class="anchored" data-anchor-id="通俗理解给-ai-装上眼睛">1.1 通俗理解：给 AI 装上”眼睛”</h3>
<p>想象一下，如果你能和一台机器这样对话：</p>
<blockquote class="blockquote">
<p><strong>你</strong>：“请描述这张图片里有什么？”<br>
<strong>AI</strong>：“图中有一只金毛犬在草地上奔跑，背景是蓝天白云，远处还有几棵树。”</p>
</blockquote>
<p>或者更复杂一点：</p>
<blockquote class="blockquote">
<p><strong>你</strong>：“这张图表显示了什么趋势？”<br>
<strong>AI</strong>：“这张柱状图显示了 2020-2024 年公司营收的逐年增长趋势，其中 2023 年增长率最高，达到 35%。”</p>
</blockquote>
<p>这就是视觉语言模型（VLM）的能力——<strong>它能够同时处理视觉信息（图像、视频）和语言信息（文本），并在两者之间建立联系</strong>。</p>
<p>根据 IBM 的定义，视觉语言模型是将<strong>计算机视觉（CV）</strong>与<strong>自然语言处理（NLP）</strong>功能融合到统一系统中的多模态模型。简单来说，VLM = 视觉理解 + 语言理解。</p>
</section>
<section id="技术定义多模态-ai-的集大成者" class="level3">
<h3 class="anchored" data-anchor-id="技术定义多模态-ai-的集大成者">1.2 技术定义：多模态 AI 的集大成者</h3>
<p>从技术角度看，VLM（也常被称为 Large Vision-Language Models，LVLMs）通常由三个核心组件构成：</p>
<p><img src="../assets/images/allVLM/image2.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
<p><strong>三个核心组件的作用：</strong></p>
<ol type="1">
<li><p><strong>视觉编码器（Visual Encoder）</strong>：负责从图像中提取视觉特征。通常使用 Vision Transformer（ViT）架构，如 CLIP-ViT、EVA-CLIP 等。它将输入的图像转换为模型可以理解的”视觉 token”序列。</p></li>
<li><p><strong>视觉-语言适配器（Vision-Language Adapter）</strong>：作为视觉和语言之间的”桥梁”，将视觉特征映射到大语言模型的嵌入空间。常见设计包括：</p>
<ul>
<li>单层线性投影（如 LLaVA）</li>
<li>带非线性的 MLP（如 MiniGPT-4）</li>
<li>Q-Former 架构（如 BLIP-2）</li>
</ul></li>
<li><p><strong>大语言模型（LLM）</strong>：负责理解视觉和文本信息，并生成自然语言响应。通常使用预训练的大语言模型，如 LLaMA、Vicuna、Qwen 等。</p></li>
</ol>
</section>
<section id="vlm-与纯视觉模型纯语言模型的区别" class="level3">
<h3 class="anchored" data-anchor-id="vlm-与纯视觉模型纯语言模型的区别">1.3 VLM 与纯视觉模型、纯语言模型的区别</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 23%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th>模型类型</th>
<th>输入</th>
<th>输出</th>
<th>典型任务</th>
<th>代表模型</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>纯视觉模型</strong></td>
<td>图像</td>
<td>类别/检测框</td>
<td>图像分类、目标检测</td>
<td>ResNet、ViT、YOLO</td>
</tr>
<tr class="even">
<td><strong>纯语言模型</strong></td>
<td>文本</td>
<td>文本</td>
<td>文本生成、问答</td>
<td>GPT、LLaMA、Qwen</td>
</tr>
<tr class="odd">
<td><strong>视觉语言模型</strong></td>
<td>图像+文本</td>
<td>文本</td>
<td>图像描述、视觉问答、图文检索</td>
<td>CLIP、BLIP、LLaVA</td>
</tr>
</tbody>
</table>
<p>VLM 的核心优势在于<strong>跨模态理解能力</strong>：它不仅能识别图像中的物体，还能理解物体之间的关系、场景的语义信息，并用自然语言表达出来。</p>
<hr>
</section>
</section>
<section id="二vlm-的发展历程从萌芽到爆发" class="level2">
<h2 class="anchored" data-anchor-id="二vlm-的发展历程从萌芽到爆发">二、VLM 的发展历程：从萌芽到爆发</h2>
<section id="早期探索2019-2020双塔结构的兴起" class="level3">
<h3 class="anchored" data-anchor-id="早期探索2019-2020双塔结构的兴起">2.1 早期探索（2019-2020）：双塔结构的兴起</h3>
<p>VLM 的发展可以追溯到 2019-2020 年，这一时期的研究主要采用<strong>双塔架构（Dual-stream Architecture）</strong>，即分别用独立的编码器处理视觉和语言信息，然后在共享空间中对齐。</p>
<p><strong>代表性工作：</strong></p>
<ul>
<li><strong>ViLBERT</strong>（2019）：将 BERT 架构扩展到视觉-语言领域，采用双流架构分别处理图像和文本</li>
<li><strong>VisualBERT</strong>（2020）：首个图像-文本预训练模型，使用 Faster R-CNN 提取视觉特征</li>
<li><strong>UNITER</strong>（2020）：大规模视觉-语言预训练，在多个下游任务上取得 SOTA</li>
</ul>
<p>这些早期模型的共同特点是： - 使用目标检测器（如 Faster R-CNN）提取区域特征 - 采用单流或双流 Transformer 进行跨模态融合 - 预训练任务主要包括掩码语言建模（MLM）和图像-文本匹配（ITM）</p>
</section>
<section id="clip-时代2021对比学习的革命" class="level3">
<h3 class="anchored" data-anchor-id="clip-时代2021对比学习的革命">2.2 CLIP 时代（2021）：对比学习的革命</h3>
<p>2021 年，OpenAI 发布了 <strong>CLIP（Contrastive Language-Image Pre-training）</strong>，这是 VLM 发展史上的里程碑事件。</p>
<p><strong>CLIP 的核心创新：</strong></p>
<ol type="1">
<li><strong>对比学习范式</strong>：通过最大化匹配图文对的相似度、最小化不匹配图文对的相似度来学习跨模态表示</li>
<li><strong>双塔架构</strong>：图像编码器和文本编码器独立工作，输出映射到同一语义空间</li>
<li><strong>大规模数据</strong>：使用 4 亿个互联网收集的图像-文本对进行训练</li>
</ol>
<p><img src="../assets/images/allVLM/image3.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
<p>CLIP 的革命性意义在于： - <strong>零样本能力</strong>：无需微调即可应用于下游任务 - <strong>开放词汇</strong>：不受预定义类别限制，可识别任意概念 - <strong>强大的迁移能力</strong>：作为视觉编码器广泛应用于后续 VLM</p>
<p>同期，Google 发布了 <strong>ALIGN</strong>，采用类似的对比学习思路，但使用 18 亿个噪声更大的图文对进行训练，进一步验证了规模的重要性。</p>
</section>
<section id="统一框架2022理解与生成的一体化" class="level3">
<h3 class="anchored" data-anchor-id="统一框架2022理解与生成的一体化">2.3 统一框架（2022）：理解与生成的一体化</h3>
<p>2022 年，Salesforce 发布了 <strong>BLIP（Bootstrapping Language-Image Pre-training）</strong>，提出了统一的视觉-语言理解与生成框架。</p>
<p><strong>BLIP 的核心贡献：</strong></p>
<ol type="1">
<li><strong>MED（Multimodal Mixture of Encoder-Decoder）架构</strong>：
<ul>
<li>单模态编码器：分别编码图像和文本</li>
<li>图像条件文本编码器：融合视觉信息进行理解</li>
<li>图像条件文本解码器：生成图像描述</li>
</ul></li>
<li><strong>CapFilt（Captioning and Filtering）</strong>：
<ul>
<li>使用”生成器+过滤器”策略清洗网络数据</li>
<li>生成器（Captioner）为网络图像生成合成描述</li>
<li>过滤器（Filter）去除噪声描述</li>
</ul></li>
</ol>
<p>BLIP 在图像-文本检索、图像描述、视觉问答等任务上取得了 SOTA 性能，并展示了强大的零样本迁移能力。</p>
</section>
<section id="大模型时代2023llm-赋能的视觉助手" class="level3">
<h3 class="anchored" data-anchor-id="大模型时代2023llm-赋能的视觉助手">2.4 大模型时代（2023）：LLM 赋能的视觉助手</h3>
<p>2023 年是 VLM 的爆发之年，核心趋势是<strong>将预训练的大语言模型（LLM）与视觉能力相结合</strong>。</p>
<p><strong>代表性模型：</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 20%">
<col style="width: 31%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>模型</th>
<th>机构</th>
<th>核心特点</th>
<th>发布时间</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>BLIP-2</strong></td>
<td>Salesforce</td>
<td>冻结视觉编码器和 LLM，使用 Q-Former 桥接</td>
<td>2023.01</td>
</tr>
<tr class="even">
<td><strong>LLaVA</strong></td>
<td>威斯康星大学</td>
<td>视觉指令微调，端到端训练</td>
<td>2023.04</td>
</tr>
<tr class="odd">
<td><strong>MiniGPT-4</strong></td>
<td>KAUST</td>
<td>单投影层对齐，两阶段训练</td>
<td>2023.04</td>
</tr>
<tr class="even">
<td><strong>Qwen-VL</strong></td>
<td>阿里巴巴</td>
<td>多语言、多图像、细粒度理解</td>
<td>2023.08</td>
</tr>
</tbody>
</table>
<p><strong>这一时期的共同特点：</strong></p>
<ol type="1">
<li><strong>冻结预训练模型</strong>：视觉编码器和 LLM 保持冻结，仅训练适配器，大幅降低训练成本</li>
<li><strong>指令微调</strong>：使用指令数据对模型进行微调，提升指令跟随能力</li>
<li><strong>端到端训练</strong>：从视觉输入直接生成文本输出，简化流程</li>
</ol>
<p>以 <strong>LLaVA</strong> 为例，它首次将<strong>视觉指令微调（Visual Instruction Tuning）</strong>引入 VLM： - 使用 GPT-4 生成多模态指令跟随数据 - 将 CLIP 视觉编码器与 Vicuna LLM 连接 - 端到端微调，实现强大的多模态对话能力</p>
<p><strong>BLIP-2</strong> 则提出了两阶段训练策略： - 第一阶段：使用冻结的视觉编码器训练 Q-Former 进行视觉-语言表示学习 - 第二阶段：将 Q-Former 连接到冻结的 LLM，进行视觉到语言的生成学习</p>
</section>
<section id="最新进展2024-2025多模态原生大模型" class="level3">
<h3 class="anchored" data-anchor-id="最新进展2024-2025多模态原生大模型">2.5 最新进展（2024-2025）：多模态原生大模型</h3>
<p>进入 2024-2025 年，VLM 朝着<strong>更大规模、更强能力、更多模态</strong>的方向发展。</p>
<p><strong>主要趋势：</strong></p>
<ol type="1">
<li><strong>模型规模扩大</strong>：从 7B 参数扩展到 70B+ 参数</li>
<li><strong>分辨率提升</strong>：支持更高分辨率输入（如 448×448、896×896）</li>
<li><strong>多模态扩展</strong>：从图像-文本扩展到视频、音频等多模态</li>
<li><strong>原生多模态架构</strong>：不再简单拼接单模态模型，而是从头设计多模态架构</li>
</ol>
<p>以 <strong>Qwen2-VL</strong> 为例，它引入了： - 原生动态分辨率支持，可处理任意分辨率的图像 - 多模态旋转位置编码（M-RoPE），统一处理图像和视频 - 强大的文档理解和 OCR 能力</p>
<hr>
</section>
</section>
<section id="三vlm-的核心技术原理" class="level2">
<h2 class="anchored" data-anchor-id="三vlm-的核心技术原理">三、VLM 的核心技术原理</h2>
<section id="模型架构单流-vs-双流" class="level3">
<h3 class="anchored" data-anchor-id="模型架构单流-vs-双流">3.1 模型架构：单流 vs 双流</h3>
<p>根据 VLP 综述，VLM 的架构可以从两个维度进行分类：</p>
<section id="从多模态融合角度单流-vs-双流" class="level4">
<h4 class="anchored" data-anchor-id="从多模态融合角度单流-vs-双流">3.1.1 从多模态融合角度：单流 vs 双流</h4>
<p><strong>单流架构（Single-stream）</strong>： - 将视觉特征和文本特征拼接后输入单个 Transformer - 使用统一的自注意力机制进行融合 - 参数效率更高，但模态间干扰较大 - 代表：VisualBERT、UNITER</p>
<p><strong>双流架构（Dual-stream）</strong>： - 视觉和文本分别输入独立的 Transformer - 通过交叉注意力（Cross-Attention）实现跨模态交互 - 模态内信息保留更好，但参数量更大 - 代表：CLIP、BLIP-2</p>
<p><img src="../assets/images/allVLM/image4.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
</section>
<section id="从整体设计角度encoder-only-vs-encoder-decoder" class="level4">
<h4 class="anchored" data-anchor-id="从整体设计角度encoder-only-vs-encoder-decoder">3.1.2 从整体设计角度：Encoder-only vs Encoder-Decoder</h4>
<p><strong>Encoder-only</strong>： - 仅使用编码器提取跨模态表示 - 适用于理解任务（如图文检索、视觉问答） - 代表：CLIP、ALBEF</p>
<p><strong>Encoder-Decoder</strong>： - 编码器提取特征，解码器生成文本 - 适用于生成任务（如图像描述） - 代表：BLIP、OFA</p>
</section>
</section>
<section id="预训练目标" class="level3">
<h3 class="anchored" data-anchor-id="预训练目标">3.2 预训练目标</h3>
<p>VLM 的预训练目标是学习视觉和语言之间的语义对应关系。根据 VLP 综述，主要预训练目标可以分为四类：</p>
<section id="补全类completion" class="level4">
<h4 class="anchored" data-anchor-id="补全类completion">3.2.1 补全类（Completion）</h4>
<p><strong>掩码语言建模（MLM）</strong>： - 随机掩码文本中的部分 token，让模型根据视觉信息和上下文预测 - 公式：<span class="math inline">\(\mathcal{L}_{MLM} = -\mathbb{E}_{(v,w)\sim D} \log P(w_m|w_{\setminus m}, v)\)</span> - 应用：VisualBERT、UNITER</p>
<p><strong>掩码视觉建模（MVM）</strong>： - 随机掩码部分图像区域，让模型根据文本和其他视觉信息预测 - 变体：掩码特征回归、掩码特征分类 - 应用：UNITER、OSCAR</p>
</section>
<section id="匹配类matching" class="level4">
<h4 class="anchored" data-anchor-id="匹配类matching">3.2.2 匹配类（Matching）</h4>
<p><strong>图像-文本对比学习（ITC）</strong>： - 拉近匹配图文对的特征距离，推远不匹配图文对 - 使用 InfoNCE 损失函数 - 公式：<span class="math inline">\(\mathcal{L}_{ITC} = -\frac{1}{2}\mathbb{E}_{(I,T)\sim D}[CE(y^{v2t}, p^{v2t}(I)) + CE(y^{t2v}, p^{t2v}(T))]\)</span> - 应用：CLIP、ALBEF、BLIP</p>
<p><strong>图像-文本匹配（ITM）</strong>： - 二分类任务，判断图文对是否匹配 - 使用难负样本挖掘策略 - 应用：ALBEF、BLIP</p>
</section>
<section id="时序类temporal" class="level4">
<h4 class="anchored" data-anchor-id="时序类temporal">3.2.3 时序类（Temporal）</h4>
<p><strong>帧序建模（FOM）</strong>： - 打乱视频帧顺序，让模型预测正确顺序 - 用于视频-语言预训练 - 应用：VideoBERT</p>
</section>
<section id="特定类型" class="level4">
<h4 class="anchored" data-anchor-id="特定类型">3.2.4 特定类型</h4>
<p><strong>视觉问答（VQA）</strong>： - 将 VQA 作为预训练任务 - 分类或生成式答案预测</p>
<p><strong>视觉描述（VC）</strong>： - 自回归生成图像描述 - 使用语言建模损失</p>
</section>
</section>
<section id="视觉特征提取的演进" class="level3">
<h3 class="anchored" data-anchor-id="视觉特征提取的演进">3.3 视觉特征提取的演进</h3>
<p>VLM 中视觉特征提取方式经历了三代演进：</p>
<section id="第一代基于目标检测的区域特征od-rfs" class="level4">
<h4 class="anchored" data-anchor-id="第一代基于目标检测的区域特征od-rfs">3.3.1 第一代：基于目标检测的区域特征（OD-RFs）</h4>
<ul>
<li>使用预训练的目标检测器（如 Faster R-CNN）提取区域特征</li>
<li>每个区域特征为 2048 维向量，附带边界框坐标</li>
<li>优点：语义明确，包含物体级信息</li>
<li>缺点：计算开销大，只能提取有限数量的区域</li>
<li>代表：VisualBERT、UNITER、OSCAR</li>
</ul>
</section>
<section id="第二代基于-cnn-的网格特征cnn-gfs" class="level4">
<h4 class="anchored" data-anchor-id="第二代基于-cnn-的网格特征cnn-gfs">3.3.2 第二代：基于 CNN 的网格特征（CNN-GFs）</h4>
<ul>
<li>使用 CNN（如 ResNet）提取网格特征</li>
<li>可以端到端训练</li>
<li>优点：计算效率更高</li>
<li>缺点：语义信息不如区域特征明确</li>
<li>代表：PixelBERT</li>
</ul>
</section>
<section id="第三代基于-vit-的-patch-特征vit-pfs" class="level4">
<h4 class="anchored" data-anchor-id="第三代基于-vit-的-patch-特征vit-pfs">3.3.3 第三代：基于 ViT 的 Patch 特征（ViT-PFs）</h4>
<ul>
<li>使用 Vision Transformer 将图像分割为 patch 序列</li>
<li>每个 patch 作为一个 token</li>
<li>优点：与 NLP 的 token 形式统一，便于跨模态融合</li>
<li>代表：CLIP、BLIP、LLaVA</li>
</ul>
<p><img src="../assets/images/allVLM/image5.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
</section>
</section>
<section id="训练策略两阶段-vs-三阶段" class="level3">
<h3 class="anchored" data-anchor-id="训练策略两阶段-vs-三阶段">3.4 训练策略：两阶段 vs 三阶段</h3>
<p>现代 VLM 通常采用多阶段训练策略：</p>
<section id="两阶段训练以-blip-2-为例" class="level4">
<h4 class="anchored" data-anchor-id="两阶段训练以-blip-2-为例">3.4.1 两阶段训练（以 BLIP-2 为例）</h4>
<p><strong>第一阶段：视觉-语言表示学习</strong> - 冻结视觉编码器 - 训练 Q-Former 提取与文本最相关的视觉特征 - 使用 ITC、ITM、ITG 三个目标</p>
<p><strong>第二阶段：视觉到语言生成学习</strong> - 将 Q-Former 连接到冻结的 LLM - 训练 Q-Former 使输出可被 LLM 理解 - 使用语言建模损失</p>
</section>
<section id="三阶段训练以-qwen-vl-为例" class="level4">
<h4 class="anchored" data-anchor-id="三阶段训练以-qwen-vl-为例">3.4.2 三阶段训练（以 Qwen-VL 为例）</h4>
<p><strong>第一阶段：预训练</strong> - 冻结 LLM，训练视觉编码器和适配器 - 使用大规模图文对数据（14 亿对） - 低分辨率输入（224×224）</p>
<p><strong>第二阶段：多任务预训练</strong> - 解锁 LLM，端到端训练 - 引入高质量细粒度数据（VQA、OCR、Grounding 等） - 高分辨率输入（448×448）</p>
<p><strong>第三阶段：监督微调</strong> - 冻结视觉编码器，微调 LLM 和适配器 - 使用指令跟随数据 - 增强对话能力和指令理解能力</p>
<hr>
</section>
</section>
</section>
<section id="四主流-vlm-模型详解" class="level2">
<h2 class="anchored" data-anchor-id="四主流-vlm-模型详解">四、主流 VLM 模型详解</h2>
<section id="clip跨模态对比学习的开创者" class="level3">
<h3 class="anchored" data-anchor-id="clip跨模态对比学习的开创者">4.1 CLIP：跨模态对比学习的开创者</h3>
<p><strong>基本信息</strong>： - 发布时间：2021 年 2 月 - 机构：OpenAI - 参数量：约 4 亿参数（最大版本） - 训练数据：4 亿图文对</p>
<p><strong>核心架构</strong>：</p>
<p><img src="../assets/images/allVLM/image6.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># CLIP 伪代码</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>image_features <span class="op">=</span> image_encoder(images)  <span class="co"># ViT 或 ResNet</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> text_encoder(texts)      <span class="co"># Transformer</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 归一化</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>image_features <span class="op">=</span> normalize(image_features)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>text_features <span class="op">=</span> normalize(text_features)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 计算相似度</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>similarity <span class="op">=</span> image_features <span class="op">@</span> text_features.T</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 对比损失</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> cross_entropy_loss(similarity, labels)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>主要能力</strong>： - 零样本图像分类 - 图像-文本检索 - 作为视觉编码器用于下游任务</p>
<p><strong>局限性</strong>： - 仅支持理解任务，不支持生成任务 - 对细粒度视觉理解能力有限</p>
</section>
<section id="blipblip-2统一理解与生成" class="level3">
<h3 class="anchored" data-anchor-id="blipblip-2统一理解与生成">4.2 BLIP/BLIP-2：统一理解与生成</h3>
<section id="blip" class="level4">
<h4 class="anchored" data-anchor-id="blip">4.2.1 BLIP</h4>
<p><strong>核心创新</strong>： - MED（多模态编解码混合）架构 - CapFilt 数据自举方法</p>
<p><strong>CapFilt 流程</strong>：</p>
<p><img src="../assets/images/allVLM/image7.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
</section>
<section id="blip-2" class="level4">
<h4 class="anchored" data-anchor-id="blip-2">4.2.2 BLIP-2</h4>
<p><strong>核心创新</strong>： - Q-Former 架构：轻量级的查询 Transformer - 两阶段训练策略 - 可插拔的冻结模型设计</p>
<p><strong>Q-Former 架构</strong>：</p>
<p><img src="../assets/images/allVLM/image8.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
<p><strong>性能表现</strong>： - 在 VQAv2 上超过 Flamingo-80B 8.7%，但可训练参数仅为其 1/54</p>
</section>
</section>
<section id="llava视觉指令微调的先锋" class="level3">
<h3 class="anchored" data-anchor-id="llava视觉指令微调的先锋">4.3 LLaVA：视觉指令微调的先锋</h3>
<p><strong>基本信息</strong>： - 发布时间：2023 年 4 月 - 机构：威斯康星大学麦迪逊分校 - 核心思想：将指令微调从 NLP 扩展到视觉-语言领域</p>
<p><strong>数据生成</strong>： - 使用 GPT-4 生成多模态指令跟随数据 - 三种类型：对话、详细描述、复杂推理 - 共 158K 样本</p>
<p><strong>架构</strong>：</p>
<p><img src="../assets/images/allVLM/image9.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
<p><strong>两阶段训练</strong>：</p>
<ol type="1">
<li><strong>特征对齐预训练</strong>：
<ul>
<li>冻结视觉编码器和 LLM</li>
<li>仅训练投影矩阵</li>
<li>使用 595K 图文对</li>
</ul></li>
<li><strong>端到端微调</strong>：
<ul>
<li>冻结视觉编码器</li>
<li>微调投影层和 LLM</li>
<li>使用 158K 指令数据</li>
</ul></li>
</ol>
<p><strong>性能</strong>： - 在 Science QA 上与 GPT-4 结合达到 92.53% 准确率（SOTA）</p>
</section>
<section id="minigpt-4极简设计的强大能力" class="level3">
<h3 class="anchored" data-anchor-id="minigpt-4极简设计的强大能力">4.4 MiniGPT-4：极简设计的强大能力</h3>
<p><strong>基本信息</strong>： - 发布时间：2023 年 4 月 - 机构：阿卜杜拉国王科技大学（KAUST）</p>
<p><strong>架构</strong>：</p>
<p><img src="../assets/images/allVLM/image10.png" alt="fork" style="width: 50%; height: auto; display: block; margin-left: auto; margin-right: auto;"></p>
<p><strong>核心发现</strong>： - 仅训练一个线性投影层即可有效对齐视觉和语言 - 训练仅需 4 张 A100 GPU，约 10 小时</p>
<p><strong>两阶段训练</strong>：</p>
<ol type="1">
<li><strong>第一阶段</strong>：使用大规模图文对（500 万对）进行预训练</li>
<li><strong>第二阶段</strong>：使用 3500 个高质量详细描述进行微调</li>
</ol>
<p><strong>涌现能力</strong>： - 详细图像描述 - 网站代码生成 - 诗歌创作 - 食谱生成 - 幽默解释</p>
</section>
<section id="qwen-vl多语言多图像的通用模型" class="level3">
<h3 class="anchored" data-anchor-id="qwen-vl多语言多图像的通用模型">4.5 Qwen-VL：多语言多图像的通用模型</h3>
<p><strong>基本信息</strong>： - 发布时间：2023 年 8 月 - 机构：阿里巴巴 - 基础模型：Qwen-7B</p>
<p><strong>核心特点</strong>：</p>
<ol type="1">
<li><strong>多语言支持</strong>：
<ul>
<li>支持中英文双语</li>
<li>训练数据中 77.3% 英文，22.7% 中文</li>
</ul></li>
<li><strong>多图像输入</strong>：
<ul>
<li>支持任意交错的图文数据</li>
<li>可比较、理解多张图像</li>
</ul></li>
<li><strong>细粒度理解</strong>：
<ul>
<li>高分辨率输入（448×448）</li>
<li>支持文本阅读（OCR）</li>
<li>支持视觉定位（Grounding）</li>
</ul></li>
</ol>
<p><strong>三阶段训练</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>阶段</th>
<th>数据</th>
<th>训练策略</th>
<th>分辨率</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>预训练</td>
<td>14 亿图文对</td>
<td>冻结 LLM，训练视觉模块</td>
<td>224×224</td>
</tr>
<tr class="even">
<td>多任务预训练</td>
<td>7 个任务（76.8M 样本）</td>
<td>端到端训练</td>
<td>448×448</td>
</tr>
<tr class="odd">
<td>监督微调</td>
<td>350K 指令数据</td>
<td>冻结视觉，微调 LLM</td>
<td>448×448</td>
</tr>
</tbody>
</table>
<p><strong>性能表现</strong>： - Flickr30K 图像描述：85.8 CIDEr（零样本 SOTA） - VQAv2：79.5% 准确率 - RefCOCO 定位：88.55% 准确率</p>
<hr>
</section>
</section>
<section id="五vlm-的应用场景" class="level2">
<h2 class="anchored" data-anchor-id="五vlm-的应用场景">五、VLM 的应用场景</h2>
<section id="图像描述与视觉问答" class="level3">
<h3 class="anchored" data-anchor-id="图像描述与视觉问答">5.1 图像描述与视觉问答</h3>
<p><strong>图像描述（Image Captioning）</strong>： - 为图像生成自然语言描述 - 应用：辅助视障人士、图像检索、内容审核</p>
<p><strong>视觉问答（Visual Question Answering, VQA）</strong>： - 根据图像内容回答自然语言问题 - 应用：智能客服、教育辅助、医疗影像分析</p>
</section>
<section id="图文检索" class="level3">
<h3 class="anchored" data-anchor-id="图文检索">5.2 图文检索</h3>
<p><strong>图像-文本检索</strong>： - 文本到图像检索：根据文本描述找到匹配图像 - 图像到文本检索：根据图像找到相关文本描述 - 应用：搜索引擎、电商平台、内容推荐</p>
</section>
<section id="文档理解" class="level3">
<h3 class="anchored" data-anchor-id="文档理解">5.3 文档理解</h3>
<p><strong>OCR 与文档分析</strong>： - 文本阅读：识别图像中的文字内容 - 表格理解：解析表格结构和内容 - 图表分析：理解图表中的数据和趋势 - 应用：金融报告分析、合同审查、发票处理</p>
</section>
<section id="视觉定位与指代表达" class="level3">
<h3 class="anchored" data-anchor-id="视觉定位与指代表达">5.4 视觉定位与指代表达</h3>
<p><strong>视觉定位（Visual Grounding）</strong>： - 根据文本描述定位图像中的物体 - 输出边界框坐标</p>
<p><strong>指代表达理解（Referring Expression Comprehension）</strong>： - 理解如”左边的红衣服女孩”这样的描述 - 应用：机器人导航、图像编辑、交互式 AI</p>
</section>
<section id="创意生成" class="level3">
<h3 class="anchored" data-anchor-id="创意生成">5.5 创意生成</h3>
<p><strong>图像到代码</strong>： - 将手绘草图转换为 HTML/CSS 代码 - 应用：前端开发、原型设计</p>
<p><strong>图像到故事/诗歌</strong>： - 根据图像创作故事或诗歌 - 应用：内容创作、教育娱乐</p>
<hr>
</section>
</section>
<section id="六幻觉问题vlm-的阿喀琉斯之踵" class="level2">
<h2 class="anchored" data-anchor-id="六幻觉问题vlm-的阿喀琉斯之踵">六、幻觉问题：VLM 的阿喀琉斯之踵</h2>
<section id="什么是-vlm-幻觉" class="level3">
<h3 class="anchored" data-anchor-id="什么是-vlm-幻觉">6.1 什么是 VLM 幻觉？</h3>
<p>根据《视觉语言大模型的幻觉综述》，<strong>VLM 幻觉是指模型生成的文本响应表现出对图像内容的错误感知</strong>。</p>
<p><strong>幻觉示例</strong>：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>图像内容</th>
<th>模型输出</th>
<th>问题</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>只有酸奶和草莓的冰箱</td>
<td>“有草莓味酸奶”</td>
<td>物体存在幻觉</td>
</tr>
<tr class="even">
<td>限速 40km/h 的路牌</td>
<td>“限速 60km/h”</td>
<td>属性幻觉</td>
</tr>
<tr class="odd">
<td>猫在沙发上</td>
<td>“沙发上有一只狗”</td>
<td>物体识别错误</td>
</tr>
</tbody>
</table>
</section>
<section id="幻觉的分类" class="level3">
<h3 class="anchored" data-anchor-id="幻觉的分类">6.2 幻觉的分类</h3>
<p>根据幻觉综述，VLM 幻觉可分为三类：</p>
<ol type="1">
<li><strong>物体存在幻觉（Object Existence Hallucination）</strong>：
<ul>
<li>描述图像中不存在的物体</li>
<li>遗漏图像中存在的物体</li>
</ul></li>
<li><strong>物体属性幻觉（Object Attribute Hallucination）</strong>：
<ul>
<li>错误描述物体的颜色、形状、材质等属性</li>
</ul></li>
<li><strong>物体关系幻觉（Object Relationship Hallucination）</strong>：
<ul>
<li>错误描述物体之间的空间位置关系</li>
<li>错误描述物体之间的交互关系</li>
</ul></li>
</ol>
<p>此外，研究还定义了其他类型的幻觉： - <strong>多模态冲突幻觉</strong>：文本输入与视觉输入不匹配时引发的幻觉 - <strong>反常识幻觉</strong>：图像中存在反常识元素时导致的幻觉 - <strong>事件幻觉</strong>：基于错误感知创造出完全虚构的事件或情景 - <strong>数字幻觉</strong>：未能准确识别图像中特定物体的数量</p>
</section>
<section id="幻觉的成因" class="level3">
<h3 class="anchored" data-anchor-id="幻觉的成因">6.3 幻觉的成因</h3>
<p>幻觉综述从四个维度分析了幻觉的成因：</p>
<section id="训练数据相关" class="level4">
<h4 class="anchored" data-anchor-id="训练数据相关">6.3.1 训练数据相关</h4>
<ol type="1">
<li><strong>图像的文本描述不够细致</strong>：
<ul>
<li>开源数据集中的文本描述较为粗糙</li>
<li>无法充分覆盖图像内容</li>
</ul></li>
<li><strong>合成的指令微调数据存在噪声</strong>：
<ul>
<li>使用 GPT-4 生成的指令数据可能包含幻觉元素</li>
<li>研究显示 LLaVA 的 GPT-4 合成数据中约 32.6% 含有幻觉</li>
</ul></li>
<li><strong>数据中存在统计偏差</strong>：
<ul>
<li>物体类别出现频率不均衡</li>
<li>正向响应比例过高</li>
</ul></li>
</ol>
</section>
<section id="训练任务相关" class="level4">
<h4 class="anchored" data-anchor-id="训练任务相关">6.3.2 训练任务相关</h4>
<ul>
<li>单一的语言建模任务缺乏对模态间一致性的显式约束</li>
<li>模型过于关注生成文本的流畅性，而忽略与视觉信息的一致性</li>
</ul>
</section>
<section id="视觉编码相关" class="level4">
<h4 class="anchored" data-anchor-id="视觉编码相关">6.3.3 视觉编码相关</h4>
<ol type="1">
<li><strong>细粒度视觉信息感知能力不足</strong>：
<ul>
<li>输入分辨率通常低于 400 像素</li>
<li>训练数据中的文本标注只能涵盖主要内容而非细节</li>
</ul></li>
<li><strong>视觉特征无法受到 LLM 的充分关注</strong>：
<ul>
<li>视觉 token 在 LLM 中的平均注意力分数不足 20%</li>
<li>随着生成进行，对视觉 token 的关注持续下降</li>
</ul></li>
</ol>
</section>
<section id="文本生成相关" class="level4">
<h4 class="anchored" data-anchor-id="文本生成相关">6.3.4 文本生成相关</h4>
<ol type="1">
<li><strong>LLM 的先验性知识偏差</strong>：
<ul>
<li>模型倾向于依赖先验知识而非实际视觉证据</li>
<li>即使输入噪声图像，仍可能输出高置信度答案</li>
</ul></li>
<li><strong>自回归解码中的偏差累积</strong>：
<ul>
<li>早期生成的小错误会在后续生成中被放大</li>
<li>幻觉频率随生成长度增加而升高</li>
</ul></li>
<li><strong>固有的采样随机性</strong>：
<ul>
<li>采样过程可能导致模型偶尔输出概率较低的错误 token</li>
</ul></li>
</ol>
</section>
</section>
<section id="幻觉的评估" class="level3">
<h3 class="anchored" data-anchor-id="幻觉的评估">6.4 幻觉的评估</h3>
<p>幻觉评估基准包括：</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>基准</th>
<th>任务形式</th>
<th>评估指标</th>
<th>幻觉类型</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CHAIR</td>
<td>图像描述</td>
<td>CHAIRi, CHAIRs</td>
<td>物体存在</td>
</tr>
<tr class="even">
<td>POPE</td>
<td>视觉问答</td>
<td>准确率、F1</td>
<td>物体存在</td>
</tr>
<tr class="odd">
<td>MME</td>
<td>视觉问答</td>
<td>Accuracy+</td>
<td>存在、属性</td>
</tr>
<tr class="even">
<td>AMBER</td>
<td>描述+问答</td>
<td>准确率、精确率、召回率</td>
<td>存在、属性、关系</td>
</tr>
<tr class="odd">
<td>MMHAL-BENCH</td>
<td>描述+问答</td>
<td>GPT-4 评分</td>
<td>多种类型</td>
</tr>
</tbody>
</table>
</section>
<section id="幻觉的治理" class="level3">
<h3 class="anchored" data-anchor-id="幻觉的治理">6.5 幻觉的治理</h3>
<p>幻觉综述从五个维度讨论了幻觉治理策略：</p>
<section id="数据侧" class="level4">
<h4 class="anchored" data-anchor-id="数据侧">6.5.1 数据侧</h4>
<ul>
<li>提升指令数据的多样性</li>
<li>增强图像-文本关联度</li>
<li>使用高质量的人工标注数据</li>
</ul>
</section>
<section id="视觉感知侧" class="level4">
<h4 class="anchored" data-anchor-id="视觉感知侧">6.5.2 视觉感知侧</h4>
<ul>
<li>使用更高分辨率的视觉编码器</li>
<li>集成多个视觉编码器</li>
<li>增加视觉编码器的参数规模</li>
</ul>
</section>
<section id="训练策略侧" class="level4">
<h4 class="anchored" data-anchor-id="训练策略侧">6.5.3 训练策略侧</h4>
<ul>
<li>引入新的监督信号</li>
<li>使用强化学习</li>
<li>增加任务信息</li>
</ul>
</section>
<section id="推理侧" class="level4">
<h4 class="anchored" data-anchor-id="推理侧">6.5.4 推理侧</h4>
<ul>
<li>对比解码</li>
<li>不确定性估计</li>
<li>多步推理</li>
</ul>
</section>
<section id="事后修正侧" class="level4">
<h4 class="anchored" data-anchor-id="事后修正侧">6.5.5 事后修正侧</h4>
<ul>
<li>使用外部知识验证</li>
<li>自我修正机制</li>
<li>人类反馈</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="七vlm-的技术挑战与未来方向" class="level2">
<h2 class="anchored" data-anchor-id="七vlm-的技术挑战与未来方向">七、VLM 的技术挑战与未来方向</h2>
<section id="当前挑战" class="level3">
<h3 class="anchored" data-anchor-id="当前挑战">7.1 当前挑战</h3>
<section id="幻觉问题" class="level4">
<h4 class="anchored" data-anchor-id="幻觉问题">7.1.1 幻觉问题</h4>
<ul>
<li>如前所述，幻觉是限制 VLM 在高风险领域应用的主要因素</li>
<li>需要更可靠的评估和治理方法</li>
</ul>
</section>
<section id="细粒度理解" class="level4">
<h4 class="anchored" data-anchor-id="细粒度理解">7.1.2 细粒度理解</h4>
<ul>
<li>当前 VLM 对细节的理解能力仍有限</li>
<li>高分辨率处理需要更多计算资源</li>
</ul>
</section>
<section id="多模态融合" class="level4">
<h4 class="anchored" data-anchor-id="多模态融合">7.1.3 多模态融合</h4>
<ul>
<li>如何更好地融合视觉、语言、音频等多种模态</li>
<li>如何处理模态间的信息冲突</li>
</ul>
</section>
<section id="计算效率" class="level4">
<h4 class="anchored" data-anchor-id="计算效率">7.1.4 计算效率</h4>
<ul>
<li>大规模 VLM 的推理成本高昂</li>
<li>需要更高效的模型压缩和加速技术</li>
</ul>
</section>
</section>
<section id="未来方向" class="level3">
<h3 class="anchored" data-anchor-id="未来方向">7.2 未来方向</h3>
<section id="原生多模态架构" class="level4">
<h4 class="anchored" data-anchor-id="原生多模态架构">7.2.1 原生多模态架构</h4>
<ul>
<li>从头设计支持多模态的架构，而非简单拼接单模态模型</li>
<li>统一处理文本、图像、视频、音频等多种模态</li>
</ul>
</section>
<section id="世界模型" class="level4">
<h4 class="anchored" data-anchor-id="世界模型">7.2.2 世界模型</h4>
<ul>
<li>构建能够理解物理世界的 VLM</li>
<li>支持因果推理和物理模拟</li>
</ul>
</section>
<section id="具身智能" class="level4">
<h4 class="anchored" data-anchor-id="具身智能">7.2.3 具身智能</h4>
<ul>
<li>将 VLM 与机器人结合</li>
<li>支持视觉导航、物体操作等任务</li>
</ul>
</section>
<section id="个性化与可解释性" class="level4">
<h4 class="anchored" data-anchor-id="个性化与可解释性">7.2.4 个性化与可解释性</h4>
<ul>
<li>支持用户个性化的 VLM</li>
<li>提供可解释的推理过程</li>
</ul>
<hr>
</section>
</section>
</section>
<section id="八总结" class="level2">
<h2 class="anchored" data-anchor-id="八总结">八、总结</h2>
<p>视觉语言模型（VLM）代表了人工智能领域的重要进展，它让机器能够同时”看懂”图像和”理解”语言，开启了人机交互的全新范式。</p>
<p><strong>从发展历程看</strong>： - 从早期的双塔架构到 CLIP 的对比学习革命 - 从 BLIP 的统一框架到 LLM 赋能的视觉助手 - VLM 正在朝着更大规模、更强能力、更多模态的方向发展</p>
<p><strong>从技术原理看</strong>： - VLM 的核心在于跨模态对齐和融合 - 预训练目标、模型架构、训练策略都在不断演进 - 幻觉问题仍是当前的主要挑战</p>
<p><strong>从应用场景看</strong>： - 图像描述、视觉问答、图文检索等基础任务已较为成熟 - 文档理解、视觉定位、创意生成等高级任务正在快速发展 - 未来将在更多领域发挥重要作用</p>
<p>对于初学者，理解 VLM 的关键在于把握<strong>跨模态对齐</strong>这一核心思想；对于技术从业者，则需要深入理解各种架构设计和训练策略的优劣。无论如何，VLM 都是一个值得持续关注的领域，它正在重新定义人机交互的方式，让我们离真正的通用人工智能更近一步。</p>
<hr>
</section>
<section id="参考资料-1" class="level2">
<h2 class="anchored" data-anchor-id="参考资料-1">参考资料</h2>
<p>[1]: <a href="https://www.ibm.com/cn-zh/think/topics/vision-language-models">什么是视觉语言模型 (VLM)？| IBM</a></p>
<p>[2]: <a href="https://github.com/IBM/vlm">VLM 说明</a></p>
<p>[3]: <a href="https://ar5iv.labs.arxiv.org/html/2202.09061">VLP: A Survey on Vision-Language Pre-training</a></p>
<p>[4]: <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision (CLIP)</a></p>
<p>[5]: <a href="https://arxiv.org/abs/2102.05918">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision (ALIGN)</a></p>
<p>[6]: <a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a></p>
<p>[7]: <a href="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning (LLaVA)</a></p>
<p>[8]: <a href="https://arxiv.org/abs/2301.12597">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></p>
<p>[9]: <a href="https://arxiv.org/abs/2409.12191">Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution</a></p>
<p>[10]: <a href="https://crad.ict.ac.cn/article/doi/10.7544/issn1000-1239.202440444">视觉语言大模型的幻觉综述：成因、评估与治理</a></p>
<hr>
<p><strong>延伸阅读</strong>： - <a href="https://encord.com/blog/vision-language-models-guide/">Guide to Vision-Language Models (VLMs) | Encord</a> - <a href="https://huggingface.co/blog/zh/vlms">视觉语言模型详解 | Hugging Face</a> - <a href="https://www.nvidia.com/en-us/glossary/vision-language-models/">What Are Vision Language Models | NVIDIA</a></p>


</section>
</section>

</main> <!-- /main -->
<button id="lang-toggle" type="button" aria-label="Toggle language"></button>
<script>
  (() => {
    const storageKey = "site-lang";
    const defaultLang = "zh";
    const button = document.getElementById("lang-toggle");
    if (!button) {
      return;
    }

    const applyLang = (lang) => {
      document.body.dataset.lang = lang;
      button.textContent = lang === "zh" ? "EN" : "中文";
      button.setAttribute("aria-label", lang === "zh" ? "Switch to English" : "切换中文");
      window.localStorage.setItem(storageKey, lang);
    };

    const stored = window.localStorage.getItem(storageKey);
    applyLang(stored || defaultLang);

    button.addEventListener("click", () => {
      const next = document.body.dataset.lang === "zh" ? "en" : "zh";
      applyLang(next);
    });
  })();
</script>
<script>
  (() => {
    const storageKey = "theme-preference";
    const getStored = () => {
      try {
        return window.localStorage.getItem(storageKey);
      } catch {
        return null;
      }
    };
    const setStored = (value) => {
      try {
        window.localStorage.setItem(storageKey, value);
      } catch {}
    };
    const currentTheme = () =>
      document.body.classList.contains("quarto-dark") ? "dark" : "light";

    const applyStored = () => {
      const stored = getStored();
      if (!stored) {
        return;
      }
      if (stored !== currentTheme() && window.quartoToggleColorScheme) {
        window.quartoToggleColorScheme();
      }
    };

    const bindToggle = () => {
      const toggles = document.querySelectorAll(".quarto-color-scheme-toggle");
      toggles.forEach((toggle) => {
        toggle.addEventListener("click", () => {
          window.setTimeout(() => setStored(currentTheme()), 0);
        });
      });
    };

    applyStored();
    bindToggle();
  })();
</script>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2026</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with Quarto</p>
</div>
  </div>
</footer>




</body></html>